{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0195a5e4",
   "metadata": {},
   "source": [
    "# Using VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fa15caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "from botocore.config import Config\n",
    "retry_config = Config(\n",
    "        region_name = 'us-east-1',\n",
    "        retries = {\n",
    "            'max_attempts': 10,\n",
    "            'mode': 'standard'\n",
    "        }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51ffdab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Set your AWS credentials explicitly\n",
    "aws_access_key_id=\"access_key_id\",\n",
    "aws_secret_access_key=\"secret_access_key\",\n",
    "service_name='bedrock-runtime', \n",
    "region_name='us-east-1'\n",
    "\n",
    "# Create a Bedrock Runtime client\n",
    "bedrock = boto3.client(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=region_name\n",
    ")\n",
    "\n",
    "# Create an S3 client using a session\n",
    "session = boto3.session.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=region_name\n",
    ")\n",
    "\n",
    "s3 = session.client('s3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "228dbad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating boto3 session by passing profile information. Profile can be parametrized depeding upon the env you are using\n",
    "profile_name = 'test-demo'\n",
    "session = boto3.session.Session(profile_name='test-demo')\n",
    "\n",
    "bedrock = session.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\" \n",
    ")\n",
    "\n",
    "# Create an S3 client using the session\n",
    "s3 = session.client('s3')\n",
    "\n",
    "\n",
    "\"\"\"\" \n",
    "btot3 provides two different client to ivoke bedrock operation.\n",
    "1. bedrock : creating and managing Bedrock models.\n",
    "2. bedrock-runtime : Running inference using Bedrock models.\n",
    "\"\"\"\n",
    "boto3_bedrock = session.client(\"bedrock\", config=retry_config)\n",
    "boto3_bedrock_runtime = session.client(\"bedrock-runtime\", config=retry_config)\n",
    "\n",
    "\n",
    "'''\n",
    "We will implement RAG architecture. The goal is to build vector store (Knowedge base to reduce hallucinations) \n",
    "so that model can refer to data we have provided.\n",
    "\n",
    "To achieve this, we need to first source data (this can be archived PDF/docs/txt/csv/anyother datastore even sql tables) \n",
    "So the pipeline will be.\n",
    " 1. Source datasets.\n",
    " 2. Update If any transformation required. \n",
    " 3. Split and create chunks. [Used in NLP. It requires optimization to get  better output.]\n",
    " 4. Create embedding using embedding modules [Can be used various modules available]\n",
    "'''\n",
    "\n",
    "EMBEDDINGS_MODEL_ID='amazon.titan-embed-text-v1'\n",
    "brrkEmbeddings = BedrockEmbeddings(model_id=EMBEDDINGS_MODEL_ID,client=boto3_bedrock_runtime,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ae5eef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\anaconda\\lib\\site-packages (3.17.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdeeea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(directory_path):\n",
    "    print(f\"Loading directory {directory_path}\")\n",
    "    loader = PyPDFDirectoryLoader(directory_path)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    "    )\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "    #avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents]) // len(documents) if documents else 0\n",
    "\n",
    "    avg_char_count_pre = avg_doc_length(documents)\n",
    "    avg_char_count_post = avg_doc_length(docs)\n",
    "    print(f'Average length among {len(documents)} documents loaded is {avg_char_count_pre} characters.')\n",
    "    print(f'After the split we have {len(docs)} documents more than the original {len(documents)}.')\n",
    "    print(f'Average length among {len(docs)} documents (after split) is {avg_char_count_post} characters.')\n",
    "    sample_embedding = np.array(brrkEmbeddings.embed_query(docs[0].page_content))\n",
    "    print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "    print(\"Size of the embedding: \", sample_embedding.shape)\n",
    "    \n",
    "    print(\"Storing in a vector store\")\n",
    "    try:\n",
    "        vector_store = FAISS.from_documents(docs, brrkEmbeddings)\n",
    "    except Exception:\n",
    "        raise Exception(\"Failed to create vector store\")\n",
    "    print(\"Created vector store\")\n",
    "    return vector_store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67dc39f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading directory C:\\Users\\RT\\OneDrive\\Desktop\\file\n",
      "Average length among 16 documents loaded is 1517 characters.\n",
      "After the split we have 35 documents more than the original 16.\n",
      "Average length among 35 documents (after split) is 744 characters.\n",
      "Sample embedding of a document chunk:  [ 0.19335938 -0.01031494  0.01507568 ...  0.23535156 -0.06787109\n",
      " -0.37890625]\n",
      "Size of the embedding:  (1536,)\n",
      "Storing in a vector store\n",
      "Created vector store\n"
     ]
    }
   ],
   "source": [
    "data_path = \"C:\\\\Users\\\\RT\\\\OneDrive\\\\Desktop\\\\file\"\n",
    "vectorartifacts=create_embeddings(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec1af732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading directory C:\\Users\\RT\\OneDrive\\Desktop\\file\n",
      "Average length among 16 documents loaded is 1517 characters.\n",
      "After the split we have 35 documents more than the original 16.\n",
      "Average length among 35 documents (after split) is 744 characters.\n",
      "Sample embedding of a document chunk:  [ 0.19335938 -0.01031494  0.01507568 ...  0.23535156 -0.06787109\n",
      " -0.37890625]\n",
      "Size of the embedding:  (1536,)\n",
      "Storing in a vector store\n",
      "Created vector store\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Vector Store: FAISS available through LangChain\n",
    "In this notebook we are using in-memory vector-store to store both the embeddings and the documents.\n",
    "In an enterprise context this could be replaced with a persistent store such as AWS OpenSearch, RDS Postgres with pgVector, ChromaDB, Pinecone or Weaviate.\n",
    "\n",
    "\"\"\"\n",
    "#vectorartifacts = create_embeddings(your_directory_path)\n",
    "# Call the function and store its return value\n",
    "vectorartifacts = create_embeddings(\"C:\\\\Users\\\\RT\\\\OneDrive\\\\Desktop\\\\file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6168e027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store path: C:\\Users\\RT/vectorstore\n",
      "Vector store got created in C:\\Users\\RT/vectorstore/16112023101133.vs\n",
      "Loaded vector store\n",
      "<langchain.vectorstores.faiss.FAISS object at 0x000001C3F16D8210>\n"
     ]
    }
   ],
   "source": [
    "def save_local_vector_store(vector_store, vector_store_path):\n",
    "    time_now = datetime.now().strftime(\"%d%m%Y%H%M%S\")\n",
    "    vector_store_path=vector_store_path+'/'+time_now+'.vs'\n",
    "    try:\n",
    "        if vector_store_path == \"\":\n",
    "            vector_store_path = f\"../vector_store/{time_now}.vs\"\n",
    "        os.makedirs(os.path.dirname(vector_store_path), exist_ok=True)\n",
    "        vector_store.save_local(vector_store_path)\n",
    "        with open(f\"{vector_store_path}/embeddings_model_id\", 'w') as f:\n",
    "            f.write(EMBEDDINGS_MODEL_ID)\n",
    "    except Exception:\n",
    "        print(\"Failed to save vector store, continuing without saving...\")\n",
    "    return vector_store_path\n",
    "\n",
    "vector_store_path=os.getcwd()+'/'+'vectorstore'\n",
    "print(\"Vector store path: {}\".format(vector_store_path))\n",
    "\n",
    "save_local_vector_store_path=save_local_vector_store(vectorartifacts,vector_store_path)\n",
    "print(f\"Vector store got created in {save_local_vector_store_path}\")\n",
    "\n",
    "\n",
    "def load_local_vector_store(vector_store_path):\n",
    "    try:\n",
    "        with open(f\"{vector_store_path}/embeddings_model_id\", 'r') as f:\n",
    "            embeddings_model_id = f.read()\n",
    "        vector_store = FAISS.load_local(vector_store_path, brrkEmbeddings)\n",
    "        print(\"Loaded vector store\")\n",
    "        return vector_store\n",
    "    except Exception:\n",
    "        print(\"Failed to load vector store, continuing creating one...\")\n",
    "        \n",
    "print(load_local_vector_store(save_local_vector_store_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63af4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
