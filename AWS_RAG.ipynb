{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb0bd250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "from botocore.config import Config\n",
    "retry_config = Config(\n",
    "        region_name = 'us-east-1',\n",
    "        retries = {\n",
    "            'max_attempts': 10,\n",
    "            'mode': 'standard'\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa743847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating boto3 session by passing profile information. Profile can be parametrized depeding upon the env you are using\n",
    "session = boto3.session.Session(profile_name='test-demo')\n",
    "\n",
    "\"\"\"\" \n",
    "btot3 provides two different client to ivoke bedrock operation.\n",
    "1. bedrock : creating and managing Bedrock models.\n",
    "2. bedrock-runtime : Running inference using Bedrock models.\n",
    "\"\"\"\n",
    "boto3_bedrock = session.client(\"bedrock\", config=retry_config)\n",
    "boto3_bedrock_runtime = session.client(\"bedrock-runtime\", config=retry_config)\n",
    "\n",
    "\n",
    "'''\n",
    "We will implement RAG architecture. The goal is to build vectore store (Knowedge base to reduce hallucinations) \n",
    "so that model can refer to data we have provided.\n",
    "\n",
    "To achieve this, we need to first source data (this can be archived PDF/docs/txt/csv/anyother datastore even sql tables) \n",
    "So the pipeline will be.\n",
    " 1. Source datasets.\n",
    " 2. Update If any transformation required. \n",
    " 3. Split and create chunks. [Used in NLP. It requires optimization to get  better output.]\n",
    " 4. Create embedding using embedding modules [Can be used various modules available]\n",
    "'''\n",
    "EMBEDDINGS_MODEL_ID='amazon.titan-embed-text-v1'\n",
    "brrkEmbeddings = BedrockEmbeddings(model_id=EMBEDDINGS_MODEL_ID,client=boto3_bedrock_runtime,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2282c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(directory_path):\n",
    "    print(f\"Loading directory {directory_path}\")\n",
    "    loader = PyPDFDirectoryLoader(directory_path)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    "    )\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "    avg_char_count_pre = avg_doc_length(documents)\n",
    "    avg_char_count_post = avg_doc_length(docs)\n",
    "    print(f'Average length among {len(documents)} documents loaded is {avg_char_count_pre} characters.')\n",
    "    print(f'After the split we have {len(docs)} documents more than the original {len(documents)}.')\n",
    "    print(f'Average length among {len(docs)} documents (after split) is {avg_char_count_post} characters.')\n",
    "    sample_embedding = np.array(brrkEmbeddings.embed_query(docs[0].page_content))\n",
    "    print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "    print(\"Size of the embedding: \", sample_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3e88285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading directory C:\\Users\\RT\\OneDrive\\Desktop\\file\n",
      "Average length among 16 documents loaded is 1517 characters.\n",
      "After the split we have 35 documents more than the original 16.\n",
      "Average length among 35 documents (after split) is 744 characters.\n",
      "Sample embedding of a document chunk:  [ 0.19335938 -0.01031494  0.01507568 ...  0.23535156 -0.06787109\n",
      " -0.37890625]\n",
      "Size of the embedding:  (1536,)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"C:\\\\Users\\\\RT\\\\OneDrive\\\\Desktop\\\\file\"\n",
    "create_embeddings(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83bc359",
   "metadata": {},
   "source": [
    "# New way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35dab06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import boto3\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "from botocore.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b42f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_config = Config(\n",
    "        region_name = 'us-east-1',\n",
    "        retries = {\n",
    "            'max_attempts': 10,\n",
    "            'mode': 'standard'\n",
    "        }\n",
    ")\n",
    "\n",
    "# Creating boto3 session by passing profile information. Profile can be parametrized depeding upon the env you are using\n",
    "session = boto3.session.Session(profile_name='test-demo')\n",
    "\n",
    "\"\"\"\" \n",
    "btot3 provides two different client to ivoke bedrock operation.\n",
    "1. bedrock : creating and managing Bedrock models.\n",
    "2. bedrock-runtime : Running inference using Bedrock models.\n",
    "\"\"\"\n",
    "boto3_bedrock = session.client(\"bedrock\", config=retry_config)\n",
    "boto3_bedrock_runtime = session.client(\"bedrock-runtime\", config=retry_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60d51b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vector store\n",
      "<langchain.vectorstores.faiss.FAISS object at 0x0000022C7A41CF90>\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_MODEL_ID='amazon.titan-embed-text-v1'\n",
    "brrkEmbeddings = BedrockEmbeddings(model_id=EMBEDDINGS_MODEL_ID,client=boto3_bedrock_runtime,)\n",
    "#bedrock_llm = Bedrock(model_id=\"ai21.j2-ultra-v1\", client=boto3_bedrock_runtime, model_kwargs={'max_tokens_to_sample':200})\n",
    "bedrock_llm = Bedrock(model_id=\"ai21.j2-ultra-v1\", client=boto3_bedrock_runtime)\n",
    "\n",
    "save_local_vector_store_path='C:\\\\Users\\\\RT/vectorstore/14112023235757.vs'\n",
    "\n",
    "def load_local_vector_store(vector_store_path):\n",
    "    try:\n",
    "        with open(f\"{vector_store_path}/embeddings_model_id\", 'r') as f:\n",
    "            embeddings_model_id = f.read()\n",
    "        vector_store = FAISS.load_local(vector_store_path, brrkEmbeddings)\n",
    "        print(\"Loaded vector store\")\n",
    "        return vector_store\n",
    "    except Exception:\n",
    "        print(\"Failed to load vector store, continuing creating one...\")\n",
    "        \n",
    "print(load_local_vector_store(save_local_vector_store_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5067da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Human: Use the following pieces of context to provide a concise answer to\n",
    "the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Assistant:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a21c6fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RetrievalQA_chain(query):\n",
    "    print(\"Connecting to bedrock\")\n",
    "    vector_store = load_local_vector_store(save_local_vector_store_path)\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 5, \"include_metadata\": True}\n",
    "    )\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=bedrock_llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "    result = chain({\"query\": query})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f90068bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to bedrock\n",
      "Loaded vector store\n",
      "\n",
      "From the provided context, LLaMA and T5 by Google,  Einstein by Salesforce\n"
     ]
    }
   ],
   "source": [
    "query = \"give names of LLM available to create QA app\"\n",
    "result=create_RetrievalQA_chain(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0deecca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f40dfa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
